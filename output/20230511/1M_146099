
[00:00:00.000 --> 00:00:02.000]  (早委請下唐鳳部長)
[00:00:02.000 --> 00:00:03.000]  好 唐部長
[00:00:03.000 --> 00:00:07.700]  委員長
[00:00:07.700 --> 00:00:12.600]  (部長好 現在Chart GPT它的發展非常的快速)
[00:00:12.600 --> 00:00:18.500]  (應用也多元 不管是相關的文本的一個生成)
[00:00:18.500 --> 00:00:24.500]  (不管是報告 新聞 劇本 甚至是翻譯或校對)
[00:00:24.500 --> 00:00:26.200]  (它都有這樣的一個功能)
[00:00:26.200 --> 00:00:30.700]  (或者是它也可以作為程式語言的一個編寫)
[00:00:30.700 --> 00:00:34.200]  (透過指令 它還可以寫相關的程式語言)
[00:00:34.200 --> 00:00:41.200]  (甚至Google它還通過Google的一個初級工程師的一個考試)
[00:00:41.200 --> 00:00:44.200]  (另外它也是重要的資訊的助理)
[00:00:44.200 --> 00:00:49.000]  (不管是你行程的安排 或是相關的工作流程)
[00:00:49.000 --> 00:00:54.500]  (等等它都逐步的扮演這樣的角色 逐步的成熟)
[00:00:54.500 --> 00:00:59.500]  (但是它衍生出相關的個資跟資安的問題)
[00:00:59.500 --> 00:01:03.500]  (說不還是有重要的一個把關的責任)
[00:01:03.500 --> 00:01:06.000]  (那你們初步的作為是什麼 來簡單說明一下)
[00:01:06.000 --> 00:01:09.000]  好的 謝謝 謝謝委員給我這個機會說明
[00:01:09.000 --> 00:01:12.500]  首先當然像我剛剛提到 我自己寫程式
[00:01:12.500 --> 00:01:15.500]  也有用這種生成式的AI 也有用語言模型
[00:01:15.500 --> 00:01:18.500]  但是我所使用的模型是完全跑在那台筆電裡面
[00:01:18.500 --> 00:01:20.500]  它完全沒有連到網際網路
[00:01:20.500 --> 00:01:23.000]  所以就可以確保說 我不管寫什麼程式
[00:01:23.000 --> 00:01:27.000]  只要在那台電腦裡面 不會被攔截或者竊聽等等
[00:01:27.000 --> 00:01:29.000]  那像這樣子的 我們叫做地端的
[00:01:29.000 --> 00:01:31.500]  就是在自己可以掌控的機器上部署
[00:01:31.500 --> 00:01:32.500]  這個是一件非常重要的事情
[00:01:32.500 --> 00:01:35.000]  部長重視個人的一個資安的一個保護
[00:01:35.000 --> 00:01:36.500]  個人的一個資料
[00:01:36.500 --> 00:01:40.000]  但是我們從實際的運用來看
[00:01:40.000 --> 00:01:43.000]  世界各國都有出現一些問題
[00:01:43.000 --> 00:01:45.000]  像我們以日本來看
[00:01:45.000 --> 00:01:51.000]  日本他們神奈川縣 這個橫須賀市的市政府
[00:01:51.000 --> 00:01:55.000]  他們已經開始用GPT來協助處理公務
[00:01:55.000 --> 00:02:00.000]  那台灣桃園 台南都有相關的基本的運用
[00:02:00.000 --> 00:02:05.000]  法國也有醫生透過這樣的模組來協助撰寫病例
[00:02:05.000 --> 00:02:07.000]  但是也有比較保守的國家
[00:02:07.000 --> 00:02:11.000]  像日本的鳥取縣 另外的縣鳥取縣
[00:02:11.000 --> 00:02:13.000]  它基於資安上的一個疑慮
[00:02:13.000 --> 00:02:16.000]  它是禁止公務員安裝或使用
[00:02:16.000 --> 00:02:19.000]  那美國也因為AI的這個技術
[00:02:19.000 --> 00:02:21.000]  引起資安的相關疑慮
[00:02:21.000 --> 00:02:25.000]  所以他們的商務部國家電信及資訊署
[00:02:25.000 --> 00:02:29.000]  也向民眾來徵詢相關的規範意見
[00:02:29.000 --> 00:02:33.000]  義大利也發生這種資安的一個問題
[00:02:33.000 --> 00:02:40.000]  那其實他們都有初步的一個保障跟防範的機制
[00:02:40.000 --> 00:02:42.000]  那我請教一下部長
[00:02:42.000 --> 00:02:46.000]  我們對於相關的這樣的一個CHART GPT
[00:02:46.000 --> 00:02:50.000]  你認為應該不應該禁止公務員來使用
[00:02:50.000 --> 00:02:54.000]  另外就是對於我們相關的使用的過程
[00:02:54.000 --> 00:03:00.000]  要不要數位部來主動徵詢相關民眾的看法跟意見
[00:03:00.000 --> 00:03:02.000]  另外對於各自的保護
[00:03:02.000 --> 00:03:07.000]  像我們的用戶姓名 卡號的默示碼
[00:03:07.000 --> 00:03:12.000]  以及與AI對話的標題等內容的外洩等等
[00:03:12.000 --> 00:03:13.000]  這樣的狀況
[00:03:13.000 --> 00:03:16.000]  我們有沒有什麼具體的建議跟做法
[00:03:16.000 --> 00:03:17.000]  來 說明一下
[00:03:17.000 --> 00:03:18.000]  謝謝
[00:03:18.000 --> 00:03:21.000]  我們在像委員這個簡報右下角
[00:03:21.000 --> 00:03:25.000]  義大利確實之前有一些希望CHART GPT調整的
[00:03:25.000 --> 00:03:26.000]  後來他們有調整了
[00:03:26.000 --> 00:03:29.000]  所以這個禁令就是後來有解除
[00:03:29.000 --> 00:03:30.000]  是義大利 那我們呢
[00:03:30.000 --> 00:03:32.000]  所以我們也是一樣的
[00:03:32.000 --> 00:03:34.000]  就是在國科會的這個召集之下
[00:03:34.000 --> 00:03:39.000]  我們公務員怎麼用CHART GPT這一類的生成式的AI
[00:03:39.000 --> 00:03:42.000]  大概有兩個很重要的原則我跟委員分享
[00:03:42.000 --> 00:03:43.000]  來 請說
[00:03:43.000 --> 00:03:44.000]  一個是說
[00:03:44.000 --> 00:03:47.000]  我們現在運用這些生成式的AI
[00:03:47.000 --> 00:03:49.000]  它生成出來的東西
[00:03:49.000 --> 00:03:52.000]  跟Social引擎是找到已經有的東西不一樣
[00:03:52.000 --> 00:03:54.000]  它會自己無中生有講一些
[00:03:54.000 --> 00:03:56.000]  那這個無中生有有時候是沒問題的
[00:03:56.000 --> 00:03:58.000]  但是很多時候是有問題的
[00:03:58.000 --> 00:04:00.000]  就是它自以為是很有道理
[00:04:00.000 --> 00:04:03.000]  但是其實是完全事實上不正確
[00:04:03.000 --> 00:04:05.000]  所以我們處理公務要很清楚的知道
[00:04:05.000 --> 00:04:08.000]  不能直接拿它做出來這個內容
[00:04:08.000 --> 00:04:11.000]  來當作我們公務上面來使用
[00:04:11.000 --> 00:04:13.000]  直接給出去或直接做成判斷等等
[00:04:13.000 --> 00:04:15.000]  這個是不可以的
[00:04:15.000 --> 00:04:17.000]  因為這個技術本身的特性
[00:04:17.000 --> 00:04:19.000]  我們有一些未公開的資料
[00:04:19.000 --> 00:04:22.000]  或者是一些未去識別化的相關的資料
[00:04:22.000 --> 00:04:24.000]  如果公務機關來使用
[00:04:24.000 --> 00:04:26.000]  它還是有可能產生這種
[00:04:26.000 --> 00:04:28.000]  各資資安的一個問題
[00:04:28.000 --> 00:04:30.000]  第二個部分就是各資的問題
[00:04:30.000 --> 00:04:32.000]  那各資的問題有一個解決方案
[00:04:32.000 --> 00:04:33.000]  就是像我剛剛講
[00:04:33.000 --> 00:04:37.000]  完全在你可以掌控的地端的這種機器上面來使用
[00:04:37.000 --> 00:04:38.000]  這樣子的技術
[00:04:38.000 --> 00:04:42.000]  那這個國科會目前也有做可信任的AI對話引擎
[00:04:42.000 --> 00:04:44.000]  這樣子一個模型應該很快就可以
[00:04:44.000 --> 00:04:46.000]  讓這個問題開來
[00:04:46.000 --> 00:04:50.000]  相關的Chat GPT的相關的這種生成式的AI
[00:04:50.000 --> 00:04:53.000]  以及相關的外掛的這一些軟體
[00:04:53.000 --> 00:04:56.000]  它是會產生這種隱私的問題
[00:04:56.000 --> 00:04:59.000]  詐欺的問題以及情色暴力的問題
[00:04:59.000 --> 00:05:02.000]  這個不管是民眾的個資
[00:05:02.000 --> 00:05:05.000]  或者是相關政府公務的運用
[00:05:05.000 --> 00:05:08.000]  或是產業的這一些問題
[00:05:08.000 --> 00:05:11.000]  我們都必須要注重它的安全性
[00:05:11.000 --> 00:05:15.000]  像Syber Heaven他們的有相關的統計
[00:05:15.000 --> 00:05:17.000]  他們旗下的企業用戶
[00:05:17.000 --> 00:05:20.000]  160名的員工他們去統計
[00:05:20.000 --> 00:05:24.000]  有8.2在職場有去使用
[00:05:24.000 --> 00:05:29.000]  3.1有將不能外流的這些資料上傳
[00:05:29.000 --> 00:05:33.000]  那這些都是以企業的機密文件為多
[00:05:33.000 --> 00:05:34.000]  其次是顧客的資料
[00:05:34.000 --> 00:05:37.000]  那三星電子一開始它也是開放
[00:05:37.000 --> 00:05:38.000]  現在也是禁用了吧
[00:05:38.000 --> 00:05:41.000]  好像在5月開始就禁令他們員工
[00:05:41.000 --> 00:05:43.000]  來相關的使用
[00:05:43.000 --> 00:05:44.000]  因為這一些過程
[00:05:44.000 --> 00:05:47.000]  我們必須要掌握一個重點就是說
[00:05:47.000 --> 00:05:51.000]  我們的政府公務員將非公開的資訊
[00:05:51.000 --> 00:05:54.000]  或者是未去識別化的資料
[00:05:54.000 --> 00:05:57.000]  透過這個Chat GPT
[00:05:57.000 --> 00:06:02.000]  讓導致民眾或政府的資料外洩
[00:06:02.000 --> 00:06:03.000]  這是一個嚴重的問題
[00:06:03.000 --> 00:06:07.000]  另外我們數位部對於這樣的一個
[00:06:07.000 --> 00:06:09.000]  個資跟資安的問題
[00:06:09.000 --> 00:06:11.000]  我們對民間對政府
[00:06:11.000 --> 00:06:13.000]  你有什麼具體的建議
[00:06:13.000 --> 00:06:15.000]  其實像委員這一張簡報
[00:06:15.000 --> 00:06:17.000]  就中整的非常好
[00:06:17.000 --> 00:06:20.000]  就是說本來企業的機密資料
[00:06:20.000 --> 00:06:22.000]  就不應該不管是Chat GPT也好
[00:06:22.000 --> 00:06:24.000]  或者是社群引擎也好
[00:06:24.000 --> 00:06:26.000]  或者是個人使用的Email帳號也好
[00:06:26.000 --> 00:06:29.000]  本來就不應該流到這些外面的地方
[00:06:29.000 --> 00:06:30.000]  那現在問題只是說
[00:06:30.000 --> 00:06:33.000]  因為很多人覺得語言模型真的很方便
[00:06:33.000 --> 00:06:35.000]  所以他就什麼都跟那個語言模型講
[00:06:35.000 --> 00:06:37.000]  他產生一種依賴性
[00:06:37.000 --> 00:06:38.000]  那我們解決這個的方式
[00:06:38.000 --> 00:06:41.000]  就是要讓在企業內部部署
[00:06:41.000 --> 00:06:44.000]  所謂地端的這些語言模型的功能
[00:06:44.000 --> 00:06:47.000]  可以很快的到跟Chat GPT一樣強
[00:06:47.000 --> 00:06:49.000]  那這樣子的話他就既不會外洩
[00:06:49.000 --> 00:06:50.000]  因為沒有傳到外面
[00:06:50.000 --> 00:06:52.000]  但是又可以做到語言模型
[00:06:52.000 --> 00:06:56.000]  我從短期跟長期的一個具體的建議
[00:06:56.000 --> 00:06:59.000]  那我們短期是應該要加強
[00:06:59.000 --> 00:07:01.000]  安全性的指引
[00:07:01.000 --> 00:07:05.000]  那長期我們還是要建立相關的一個規範
[00:07:05.000 --> 00:07:07.000]  那目前行政院目前的框架
[00:07:07.000 --> 00:07:09.000]  數位部的任務
[00:07:09.000 --> 00:07:12.000]  是要負責AI法制的議題中
[00:07:12.000 --> 00:07:16.000]  去制定AI產品測試規範及測試的驗證
[00:07:16.000 --> 00:07:19.000]  那目前的進度呢
[00:07:19.000 --> 00:07:22.000]  目前我們在自通安全研究院
[00:07:22.000 --> 00:07:24.000]  已經有專門的人力來做這件事情
[00:07:24.000 --> 00:07:27.000]  也跟之前工研院等等的法律來合作
[00:07:27.000 --> 00:07:30.000]  做研究什麼時候會有初步的成果出來
[00:07:30.000 --> 00:07:33.000]  我想因為國科會這邊可信任對話引擎
[00:07:33.000 --> 00:07:36.000]  是在應該預計年底會推出
[00:07:36.000 --> 00:07:37.000]  年底前啦
[00:07:37.000 --> 00:07:39.000]  那所以我想在這個中間的開發版本
[00:07:39.000 --> 00:07:41.000]  我們就會一起來驗測
[00:07:41.000 --> 00:07:43.000]  部長我們短期應該還是要
[00:07:43.000 --> 00:07:45.000]  訂定安全的指引
[00:07:45.000 --> 00:07:49.000]  就是參考國科會的人工智慧研發的一個發展指引
[00:07:49.000 --> 00:07:53.000]  另外訂定人工智慧使用的安全指引
[00:07:53.000 --> 00:07:55.000]  作為一個過度的措施
[00:07:55.000 --> 00:07:57.000]  那我想應該啊
[00:07:57.000 --> 00:07:59.000]  要舉辦公聽會跟教育講座
[00:07:59.000 --> 00:08:04.000]  要讓社會去形成AI使用的安全意識
[00:08:04.000 --> 00:08:07.000]  那長期的目標當然是要完善一些
[00:08:07.000 --> 00:08:09.000]  基本法令的一個規範
[00:08:09.000 --> 00:08:13.000]  然後明定AI產品的安全標準跟規範
[00:08:13.000 --> 00:08:17.000]  那建立機制個別測試的驗證AI產品的安全性
[00:08:17.000 --> 00:08:19.000]  來 部長簡單說我一下
[00:08:19.000 --> 00:08:21.000]  好 我覺得這都非常好
[00:08:21.000 --> 00:08:22.000]  在公部門的使用指引
[00:08:22.000 --> 00:08:24.000]  如同我今天專案報告
[00:08:24.000 --> 00:08:25.000]  國科會已經有研擬
[00:08:25.000 --> 00:08:27.000]  那我們會全力在資安上配合
[00:08:27.000 --> 00:08:30.000]  然後委員剛剛提到就是聚集民眾的意見
[00:08:30.000 --> 00:08:33.000]  這個我們在策略思考的一個叫點子鬆的活動
[00:08:33.000 --> 00:08:36.000]  會在六月到九月來進行這個意見徵集
[00:08:36.000 --> 00:08:38.000]  那最後測試驗證的規範
[00:08:38.000 --> 00:08:39.000]  這個是由資安院再出行的
[00:08:39.000 --> 00:08:41.000]  部長都有具體的一個時程出來
[00:08:41.000 --> 00:08:45.000]  也希望你能夠如期如此的一個完成
[00:08:45.000 --> 00:08:47.000]  謝謝委員 謝謝
[00:08:47.000 --> 00:08:49.000]  就在這裡我們接著接著做起跑步了

